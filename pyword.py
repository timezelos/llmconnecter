#!/usr/bin/env python3
"""
ä¸Ž Ollama æœ¬åœ°æ¨¡åž‹å¯¹è¯ï¼ˆæ”¯æŒå¤šæ—¥å¿—ã€style_logã€system_promptã€tail_promptï¼‰
"""
import requests, yaml, re, datetime, pathlib, argparse, sys, os

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸºæœ¬é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CFG_PATH = "config.yaml"               # YAML é…ç½®æ–‡ä»¶
LOG_DIR  = "logs"                      # èŠå¤©æ—¥å¿—ç›®å½•
os.makedirs(LOG_DIR, exist_ok=True)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LOG_RE = re.compile(r"\[.*?\]\s*(\w+):\s*(.*)", re.S)
now = lambda: datetime.datetime.now().isoformat(timespec="seconds")

def append_log(path: pathlib.Path, role: str, content: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as f:
        f.write(f"[{now()}] {role}: {content}\n")

def parse_log(path: pathlib.Path):
    if not path.exists(): return []
    role_map = {"USER": "user", "LLM": "assistant"}
    msgs = []
    for line in path.read_text(encoding="utf-8").splitlines():
        m = LOG_RE.match(line.strip())
        if m and m.group(1) in role_map:
            msgs.append({"role": role_map[m.group(1)], "content": m.group(2)})
    return msgs

def parse_style(path: pathlib.Path):
    if not path or not path.exists(): return []
    role_map = {"SYS": "system", "SYSTEM": "system",
                "USER": "user",   "LLM": "assistant"}
    msgs = []
    for line in path.read_text(encoding="utf-8").splitlines():
        m = LOG_RE.match(line.strip())
        if m:
            msgs.append({"role": role_map.get(m.group(1).upper(), "system"),
                         "content": m.group(2)})
    return msgs

def choose_log():
    logs = sorted(pathlib.Path(LOG_DIR).glob("*.log"))
    if not logs:
        return pathlib.Path(LOG_DIR) / f"{now().replace(':','-')}.log"
    print("é€‰æ‹©è¦åŠ è½½çš„èŠå¤©è®°å½•ï¼š")
    for i, fp in enumerate(logs, 1):
        print(f"  {i}. {fp.name}")
    print("  0. æ–°å»ºå¯¹è¯")
    while True:
        sel = input("ç¼–å·> ").strip()
        if sel == "0":
            return pathlib.Path(LOG_DIR) / f"{now().replace(':','-')}.log"
        if sel.isdigit() and 1 <= int(sel) <= len(logs):
            return logs[int(sel) - 1]
        print("æ— æ•ˆç¼–å·ï¼Œè¯·é‡è¯•ã€‚")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  load_cfgï¼šè¯»å‡º tail_prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_cfg(path: str):
    cfg = yaml.safe_load(open(path, encoding="utf-8"))
    ollama = cfg["character_config"]["agent_config"]["llm_configs"]["ollama_llm"]

    prompt_sec = cfg.get("prompt_config", {})
    base_url  = ollama["base_url"].rstrip("/") + "/chat/completions"
    model     = ollama["model"]
    temp      = ollama.get("temperature", 1.0)
    keep_alive= ollama.get("keep_alive", -1)
    sys_prompt= prompt_sec.get("system_prompt", "")
    style_log = prompt_sec.get("style_log", "")
    # >>> æ–°å¢ž (tail_prompt) ---------------------------
    tail_prompt = prompt_sec.get("tail_prompt", "")
    # <<<----------------------------------------------
    return base_url, model, temp, keep_alive, sys_prompt, style_log, tail_prompt
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def ask_ollama(url, model, temp, keep_alive, messages):
    payload = dict(model=model, temperature=temp,
                   keep_alive=keep_alive, messages=messages, stream=False)
    r = requests.post(url, json=payload, timeout=100000)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--log", help="æŒ‡å®šæ—¥å¿—æ–‡ä»¶")
    ap.add_argument("--new", action="store_true", help="å¿½ç•¥æ—§è®°å½•æ–°å»ºå¯¹è¯")
    args = ap.parse_args()

    log_fp = (pathlib.Path(args.log) if args.log and not args.new else
              pathlib.Path(LOG_DIR) / f"{now().replace(':','-')}.log"
              if args.new else choose_log())

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¡ main æŽ¥æ”¶ tail_prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    (endpoint, model, temp, keep_alive,
     sys_prompt, style_path, tail_prompt) = load_cfg(CFG_PATH)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    style_msgs = parse_style(pathlib.Path(style_path))
    history    = parse_log(log_fp)

    print(f"âœ… æ¨¡åž‹: {model} | æ—¥å¿—: {log_fp.name}")
    if style_msgs: print(f"ðŸŽ¨ style_log è¡Œæ•°: {len(style_msgs)}")
    if sys_prompt: print("ðŸ“Œ system_prompt å°†åœ¨é¦–è½®æ³¨å…¥")
    if tail_prompt:print("ðŸ”„ tail_prompt å°†åœ¨æ¯è½®æœ«å°¾æ³¨å…¥")

    first_turn = True
    while True:
        user = input("ä½ : ").strip()
        if user.lower() in {"exit", "quit"}: break

        history.append({"role": "user", "content": user})
        append_log(log_fp, "USER", user)

        # ç»„è£… messages
        if first_turn:
            messages = style_msgs.copy()
            if sys_prompt:
                messages.append({"role": "system", "content": sys_prompt})
            messages += history
        else:
            messages = history

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¢ æ¯è½®æœ«å°¾æ’ tail_prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if tail_prompt:
            messages_to_send = messages + [{"role": "system",
                                            "content": tail_prompt}]
        else:
            messages_to_send = messages
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        try:
            answer = ask_ollama(endpoint, model, temp,
                                keep_alive, messages_to_send)
        except Exception as e:
            print("âš ï¸  è¯·æ±‚å¤±è´¥:", e)
            history.pop(); continue

        print("LLM:", answer, "\n")
        history.append({"role": "assistant", "content": answer})
        append_log(log_fp, "LLM", answer)
        first_turn = False

    print("ðŸ‘‹ å¯¹è¯ç»“æŸï¼Œæ—¥å¿—å·²ä¿å­˜ ->", log_fp.resolve())

if __name__ == "__main__":
    main()
